"""
Scalability Analysis Module

Bu modÃ¼l, algoritmalarÄ±n farklÄ± aÄŸ boyutlarÄ±ndaki performansÄ±nÄ± analiz eder.
>1000 dÃ¼ÄŸÃ¼m desteÄŸi iÃ§in optimize edilmiÅŸtir.

Ã–zellikler:
- 100'den 2000+ dÃ¼ÄŸÃ¼me Ã¶lÃ§eklenebilirlik testi
- HafÄ±za ve CPU kullanÄ±m analizi
- KarÅŸÄ±laÅŸtÄ±rmalÄ± performans grafikleri
"""

import time
import gc
import tracemalloc
from typing import List, Dict, Any, Optional, Callable
from dataclasses import dataclass, field
import networkx as nx
import random

from src.services.graph_service import GraphService
from src.services.metrics_service import MetricsService
from src.algorithms import ALGORITHMS


@dataclass
class ScalabilityDataPoint:
    """Tek bir Ã¶lÃ§eklenebilirlik veri noktasÄ±."""
    node_count: int
    edge_count: int
    algorithm: str
    avg_time_ms: float
    std_time_ms: float
    min_time_ms: float
    max_time_ms: float
    success_rate: float
    avg_cost: float
    memory_mb: float = 0.0
    
    def to_dict(self) -> Dict:
        return {
            "node_count": self.node_count,
            "edge_count": self.edge_count,
            "algorithm": self.algorithm,
            "avg_time_ms": self.avg_time_ms,
            "std_time_ms": self.std_time_ms,
            "min_time_ms": self.min_time_ms,
            "max_time_ms": self.max_time_ms,
            "success_rate": self.success_rate,
            "avg_cost": self.avg_cost,
            "memory_mb": self.memory_mb
        }


@dataclass
class ScalabilityReport:
    """KapsamlÄ± Ã¶lÃ§eklenebilirlik raporu."""
    data_points: List[ScalabilityDataPoint]
    node_sizes: List[int]
    algorithms: List[str]
    total_time_sec: float
    
    # Analiz sonuÃ§larÄ±
    fastest_algorithm: str = ""
    most_scalable: str = ""  # En az zaman artÄ±ÅŸÄ± gÃ¶steren
    recommendations: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict:
        return {
            "data_points": [dp.to_dict() for dp in self.data_points],
            "node_sizes": self.node_sizes,
            "algorithms": self.algorithms,
            "total_time_sec": self.total_time_sec,
            "fastest_algorithm": self.fastest_algorithm,
            "most_scalable": self.most_scalable,
            "recommendations": self.recommendations
        }
    
    def get_time_by_algorithm(self, algorithm: str) -> List[float]:
        """Algoritma iÃ§in tÃ¼m node boyutlarÄ±ndaki sÃ¼releri dÃ¶ndÃ¼r."""
        return [dp.avg_time_ms for dp in self.data_points 
                if dp.algorithm == algorithm]
    
    def get_time_by_nodes(self, node_count: int) -> Dict[str, float]:
        """Belirli node sayÄ±sÄ± iÃ§in tÃ¼m algoritma sÃ¼relerini dÃ¶ndÃ¼r."""
        return {dp.algorithm: dp.avg_time_ms for dp in self.data_points 
                if dp.node_count == node_count}


class ScalabilityAnalyzer:
    """
    GeliÅŸmiÅŸ Ã–lÃ§eklenebilirlik Analiz AracÄ±
    
    Ã–zellikleri:
    - Ã‡oklu dÃ¼ÄŸÃ¼m boyutu desteÄŸi (100-2000+)
    - HafÄ±za profiling
    - Ä°statistiksel analiz
    - Ã–lÃ§eklenebilirlik Ã¶nerileri
    """
    
    def __init__(
        self,
        node_sizes: List[int] = None,
        n_repeats: int = 3,
        n_test_cases: int = 5,
        algorithms: List[str] = None,
        progress_callback: Callable[[int, int, str], None] = None
    ):
        """
        Args:
            node_sizes: Test edilecek dÃ¼ÄŸÃ¼m sayÄ±larÄ± [100, 250, 500, 1000, ...]
            n_repeats: Her test iÃ§in tekrar sayÄ±sÄ±
            n_test_cases: Her boyut iÃ§in test sayÄ±sÄ±
            algorithms: Test edilecek algoritmalar (default: hepsi)
            progress_callback: (current, total, message) -> None
        """
        self.node_sizes = node_sizes or [100, 250, 500, 750, 1000, 1500, 2000]
        self.n_repeats = n_repeats
        self.n_test_cases = n_test_cases
        self.algorithms = algorithms or list(ALGORITHMS.keys())
        self.progress_callback = progress_callback
    
    def run_analysis(self) -> ScalabilityReport:
        """
        Tam Ã¶lÃ§eklenebilirlik analizi Ã§alÄ±ÅŸtÄ±r.
        
        Returns:
            ScalabilityReport
        """
        start_time = time.time()
        data_points = []
        
        total_steps = len(self.node_sizes) * len(self.algorithms)
        current_step = 0
        
        for n_nodes in self.node_sizes:
            # Graf oluÅŸtur
            self._emit_progress(current_step, total_steps, 
                               f"Graf oluÅŸturuluyor ({n_nodes} dÃ¼ÄŸÃ¼m)...")
            
            graph, edge_count = self._create_test_graph(n_nodes)
            
            for algo_key in self.algorithms:
                current_step += 1
                algo_name = ALGORITHMS[algo_key][0]
                
                self._emit_progress(current_step, total_steps,
                                   f"{n_nodes} dÃ¼ÄŸÃ¼m - {algo_name}")
                
                # Algoritma testi
                dp = self._test_algorithm(graph, algo_key, n_nodes, edge_count)
                data_points.append(dp)
                
                # HafÄ±za temizle
                gc.collect()
        
        total_time = time.time() - start_time
        
        # Rapor oluÅŸtur
        report = ScalabilityReport(
            data_points=data_points,
            node_sizes=self.node_sizes,
            algorithms=[ALGORITHMS[k][0] for k in self.algorithms],
            total_time_sec=total_time
        )
        
        # Analizi tamamla
        self._analyze_results(report)
        
        return report
    
    def _create_test_graph(self, n_nodes: int) -> tuple:
        """Test iÃ§in graf oluÅŸtur."""
        # Seyreklik ayarla - bÃ¼yÃ¼k graflarda daha seyrek
        if n_nodes <= 250:
            p = 0.15
        elif n_nodes <= 500:
            p = 0.08
        elif n_nodes <= 1000:
            p = 0.04
        else:
            p = 0.02  # >1000 dÃ¼ÄŸÃ¼m iÃ§in Ã§ok seyrek
        
        service = GraphService(seed=42)  # Reproducibility
        graph = service.generate_graph(n_nodes=n_nodes, p=p)
        edge_count = graph.number_of_edges()
        
        return graph, edge_count
    
    def _test_algorithm(
        self, 
        graph: nx.Graph, 
        algo_key: str,
        n_nodes: int,
        edge_count: int
    ) -> ScalabilityDataPoint:
        """Tek algoritma iÃ§in test Ã§alÄ±ÅŸtÄ±r."""
        algo_name, AlgoClass = ALGORITHMS[algo_key]
        times = []
        costs = []
        successes = 0
        memory_peak = 0.0
        
        nodes = list(graph.nodes())
        
        for _ in range(self.n_test_cases):
            # Rastgele kaynak-hedef Ã§ifti
            source, dest = random.sample(nodes, 2)
            
            for _ in range(self.n_repeats):
                try:
                    # HafÄ±za izleme baÅŸlat
                    tracemalloc.start()
                    
                    start = time.time()
                    algo = AlgoClass(graph=graph, seed=None)
                    result = algo.optimize(
                        source=source,
                        destination=dest,
                        weights={'delay': 0.33, 'reliability': 0.33, 'resource': 0.34}
                    )
                    elapsed_ms = (time.time() - start) * 1000
                    
                    # HafÄ±za kullanÄ±mÄ±
                    current, peak = tracemalloc.get_traced_memory()
                    tracemalloc.stop()
                    memory_peak = max(memory_peak, peak / (1024 * 1024))  # MB
                    
                    times.append(elapsed_ms)
                    
                    if hasattr(result, 'path') and result.path:
                        successes += 1
                        if hasattr(result, 'fitness'):
                            costs.append(result.fitness)
                        elif hasattr(result, 'weighted_cost'):
                            costs.append(result.weighted_cost)
                            
                except Exception as e:
                    tracemalloc.stop()
                    times.append(0)
        
        # Ä°statistikler
        import numpy as np
        times_arr = np.array([t for t in times if t > 0]) if times else np.array([0])
        
        total_runs = self.n_test_cases * self.n_repeats
        
        return ScalabilityDataPoint(
            node_count=n_nodes,
            edge_count=edge_count,
            algorithm=algo_name,
            avg_time_ms=float(np.mean(times_arr)) if len(times_arr) > 0 else 0,
            std_time_ms=float(np.std(times_arr)) if len(times_arr) > 0 else 0,
            min_time_ms=float(np.min(times_arr)) if len(times_arr) > 0 else 0,
            max_time_ms=float(np.max(times_arr)) if len(times_arr) > 0 else 0,
            success_rate=successes / total_runs if total_runs > 0 else 0,
            avg_cost=float(np.mean(costs)) if costs else 0,
            memory_mb=memory_peak
        )
    
    def _analyze_results(self, report: ScalabilityReport) -> None:
        """SonuÃ§larÄ± analiz et ve Ã¶neriler oluÅŸtur."""
        if not report.data_points:
            return
        
        # En hÄ±zlÄ± algoritma (ortalama)
        algo_avg_times = {}
        for algo in report.algorithms:
            times = report.get_time_by_algorithm(algo)
            if times:
                algo_avg_times[algo] = sum(times) / len(times)
        
        if algo_avg_times:
            report.fastest_algorithm = min(algo_avg_times, key=algo_avg_times.get)
        
        # En Ã¶lÃ§eklenebilir (zaman artÄ±ÅŸ oranÄ± en dÃ¼ÅŸÃ¼k)
        scaling_factors = {}
        for algo in report.algorithms:
            times = report.get_time_by_algorithm(algo)
            if len(times) >= 2 and times[0] > 0:
                # Son / Ä°lk oranÄ±
                scaling_factors[algo] = times[-1] / times[0]
        
        if scaling_factors:
            report.most_scalable = min(scaling_factors, key=scaling_factors.get)
        
        # Ã–neriler
        report.recommendations = []
        
        if report.fastest_algorithm:
            report.recommendations.append(
                f"ğŸ† En hÄ±zlÄ± algoritma: {report.fastest_algorithm}"
            )
        
        if report.most_scalable:
            report.recommendations.append(
                f"ğŸ“ˆ En iyi Ã¶lÃ§eklenebilirlik: {report.most_scalable}"
            )
        
        # BÃ¼yÃ¼k aÄŸ Ã¶nerisi
        large_node_data = [dp for dp in report.data_points 
                          if dp.node_count >= 1000 and dp.success_rate > 0.8]
        if large_node_data:
            best_large = min(large_node_data, key=lambda x: x.avg_time_ms)
            report.recommendations.append(
                f"ğŸ”· 1000+ dÃ¼ÄŸÃ¼m iÃ§in Ã¶nerilen: {best_large.algorithm}"
            )
    
    def _emit_progress(self, current: int, total: int, message: str):
        """Progress callback Ã§aÄŸÄ±r."""
        if self.progress_callback:
            self.progress_callback(current, total, message)


__all__ = ["ScalabilityAnalyzer", "ScalabilityReport", "ScalabilityDataPoint"]
